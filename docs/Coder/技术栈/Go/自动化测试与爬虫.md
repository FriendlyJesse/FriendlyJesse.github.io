---
order: 10
---

# 自动化测试与爬虫
自动化测试一般分为**接口自动化**和**ui自动化**。
接口测试一般校验的是客户端与服务端交互的部分，校验业务逻辑以及服务端返回的数据。而ui自动化也就是ui测试则是站在**用户的角度**上通过对ui界面元素得操作来验证系统的功能。
一个是无形的，一个是可见的。

## Selenium
Selenium 是一个用于网站程序自动化的工具，它可以直接运行在浏览器中，就像真正的用户在操作一样。
### 安装 chrome 与 chrome-driver
#### 安装 chrome
依赖：
```bash
sudo apt-get update
sudo apt-get install -y curl unzip xvfb libxi6 libgconf-2-4
```
Chrome 本身：
```bash
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install ./google-chrome-stable_current_amd64.deb
```
确保它有效：
```bash
google-chrome --version
Google Chrome 123.0.6312.105
```
这里我的版本是最新版，我们需要找到对应的版本
#### 安装 chrome-driver
[ChromeDriver - WebDriver for Chrome](https://chromedriver.chromium.org/)
我们可以在其中找到 json 端点对应的 driver，然后安装
![image.png](https://cdn.nlark.com/yuque/0/2024/png/21870146/1712394462849-39d2d8cc-767d-4006-8691-059db86c93b8.png#averageHue=%23fcf9f8&clientId=u1f7baf33-c03e-4&from=paste&height=720&id=uf2256d71&originHeight=1439&originWidth=2325&originalType=binary&ratio=2&rotation=0&showTitle=false&size=336140&status=done&style=none&taskId=u3b362444-a576-4dd0-b937-7162078a15c&title=&width=1162.5)

```bash
wget https://storage.googleapis.com/chrome-for-testing-public/123.0.6312.105/linux64/chromedriver-linux64.zip
unzip chromedriver_linux64.zip
```
它是一个执行文件，可以直接通过程序驱动，或者直接运行。
#### wsl 中中文无法显示的问题
在 WSL2 打开chrome等应用程序， 界面的中文无法正常显示，具体为所有的中文都显示为方框是因为wsl2系统中缺少中文字体。
可以向系统存放字体的文件夹中添加Windows系统中的中文字体，（可以添加字体文件，不过用软连接可能会更棒些，因为节约硬盘资源）
```bash
sudo mkdir /usr/share/fonts/win11 ## to differentiate self-built font links from system font files 
sudo ln -s /mnt/c/Windows/Fonts/* /usr/share/fonts/win11
```
### 浏览器配置与启动
```go
const (
  chromeDriver = "/home/jesse/project/go-study/chromedriver"
  port         = 8080
)

func ExecAutoTest() {
  var opts = []selenium.ServiceOption{}
  selenium.SetDebug(false)
  var server, err = selenium.NewChromeDriverService(chromeDriver, port, opts...)
  if err != nil {
    log.Println("chrome driver err: ", err)
    return
  }
  defer server.Stop()

  // 连接 webdriver 服务
  //设置浏览器功能，变量caps通用于Chrome和Firefox
  caps := selenium.Capabilities{}
  // chrome.Capabilities()来自tebeka/selenium/chrome
  // 火狐使用tebeka/selenium/firefox的firefox.Capabilities{)
  // 设置Chrome的特定功能
  chromeCaps := chrome.Capabilities{
    //禁止加载图片，加快渲染速度
    Prefs: map[string]any{"profile.managed_ default_content_settings.images": 2},
    // 使用开发者调试模式
    ExcludeSwitches: []string{"enable-automation"},
    // 基本功能
    Args: []string{
      // 无界面模式，不会打开浏览器，程序后台运行
      //"--headless",
      //浏览器窗口全屏模式
      // "--start-maximized",
      //浏览器窗口大小设置
      "--window-size=1200x600",
      //取消沙盒模式
      "--no-sandbox",
      //设置请求头
      "--user-agent=Mozilla/5,0 (Windows NT 10.0; Win64;" +
      "x64) AppleWebKit/537.36 (KHTML, like Gecko) " +
      "Chrome/94.0.4606.61 Safari/537,36",
      //禁止扩展功能
      "--disable-extensions",
      //禁用沙盒模式
      "--disable-setuid-sandbox",
      // 禁止使用shm
      "--disable-dev-shm-usage",
      //禁用GPU加速
      "--disable-gpu",
      //关闭安全策略
      "m--disable-web-security",
      //允许运行不安全的内容
      "--allow-running-insecure-content",
    },
  }

  // 将谷歌浏览器特定功能 chromeCaps 添加到 caps
  caps.AddChrome(chromeCaps)
  // 设置浏览器的代理 IP
  // http := "http://"
  // caps.AddProxy(selenium.Proxy{Type: selenium.Manual, HTTP: http, HTTPPort: 0000})
  // 根据浏览器功能连接
  var urlPrefix = fmt.Sprintf("http://localhost:%d/wd/hub", port)
  var wd, _ = selenium.NewRemote(caps, urlPrefix)
  // 关闭浏览器对象
  defer wd.Quit()
  // 访问网址
  wd.Get("http://httpbin.org/get")
  pg, _ := wd.PageSource()
  fmt.Println(pg)
}
```
我们运行浏览器拿到了对应的的数据：
```go
<html><head><meta name="color-scheme" content="light dark"><meta charset="utf-8"></head><body><pre>{
  "args": {}, 
  "headers": {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7", 
    "Accept-Encoding": "gzip, deflate, br, zstd", 
    "Accept-Language": "en-US,en;q=0.9", 
    "Host": "httpbin.org", 
    "Sec-Ch-Ua": "\"Google Chrome\";v=\"123\", \"Not:A-Brand\";v=\"8\", \"Chromium\";v=\"123\"", 
    "Sec-Ch-Ua-Mobile": "?0", 
    "Sec-Ch-Ua-Platform": "\"Linux\"", 
    "Sec-Fetch-Dest": "document", 
    "Sec-Fetch-Mode": "navigate", 
    "Sec-Fetch-Site": "none", 
    "Sec-Fetch-User": "?1", 
    "Upgrade-Insecure-Requests": "1", 
    "User-Agent": "Mozilla/5,0 (Windows NT 10.0; Win64;x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.61 Safari/537,36", 
    "X-Amzn-Trace-Id": "Root=1-66110f69-2e4634ce233ef2312641f738"
  }, 
  "origin": "183.46.145.231", 
  "url": "https://httpbin.org/get"
}
</pre><div class="json-formatter-container"></div></body></html>
```
### 网页元素定位
Selenium 提供了8种网页元素定位方法。如下：

1. ByClassName 通过网页元素的 class 属性定位
2. ByID 通过网页元素的id属性定位
3. ByTagName 通过网页元素的HTML标签定位
4. ByName 通过网页元素的name定位
5. ByCSSSelector 通过css选择器定位
6. ByLinkText 和 ByPartialLinkText 通过链接文本进行定位，其中 ByPartialLinkText支持模糊匹配
7. ByXPATH 通过 XPATH 语法定位

第三方包`tebeka/selenium`通过接口方法`FindElemen()`和`FindElement()`实现元素定位。`FindElement()`查找符合定位条件的第一个元素，`FindElement()`查找符合定位条件的所有元素，并以切片形式返回。以`FindElemen()`为例讲述如何使用8种定位方法查找网页元素。
例子：
```go
// 开启 webdriver 服务
var server, _ = selenium.NewChromeDriverService(chromeDriver, port)
// 关闭服务
defer server.Stop()

var caps = selenium.Capabilities{}
// 设置 chrome 特定功能
var chromeCaps = chrome.Capabilities{
  // 使用开发者调试模式
  ExcludeSwitches: []string{"enable-automation"},
}
// 将 chrome 特定模式添加到 caps
caps.AddChrome(chromeCaps)
// 根据浏览器功能连接 Selenium
var urlPrefix = fmt.Sprintf("http://localhost:%d/wd/hub", port)
var wd, _ = selenium.NewRemote(caps, urlPrefix)
defer wd.Quit()

// 访问网址
wd.Get("https://www.baidu.com/s?wd=go")

// 通过 class 属性定位元素
var el1, _ = wd.FindElement(selenium.ByClassName, "s_ipt")
el1.SendKeys("Golang")
time.Sleep(3 * time.Second)

// 通过 id 定位元素
var el2, _ = wd.FindElement(selenium.ByID, "kw")
el2.SendKeys("good")
time.Sleep(3 * time.Second)

// 通过 标签名、className 标签定位元素，先定位局部，再定位标签
var el3, _ = wd.FindElement(selenium.ByClassName, "quickdelete-wrap")
var el31, _ = el3.FindElement(selenium.ByTagName, "input")
el31.SendKeys("nice")
time.Sleep(3 * time.Second)

// 通过 name 属性定位元素
var el4, _ = wd.FindElement(selenium.ByName, "wd")
el4.SendKeys("very")
time.Sleep(3 * time.Second)

// 通过 cssSelector 定位元素
var el5, _ = wd.FindElement(selenium.ByCSSSelector, ".s_ipt")
el5.SendKeys("go语言")
time.Sleep(3 * time.Second)

// 通过链接文本定位元素
var el6, _ = wd.FindElement(selenium.ByLinkText, "资讯")
var t6, _ = el6.Text()
fmt.Println("链接文本：", t6)

// 通过链接文本定位元素，支持模糊匹配
var el7, _ = wd.FindElement(selenium.ByPartialLinkText, "讯")
var t7, _ = el7.Text()
fmt.Println("模糊匹配链接文本：", t7)

// 通过 Xpath语法定位元素
var el8, _ = wd.FindElement(selenium.ByXPATH, `//*[@id="s_tab"]/div/b`)
var t8, _ = el8.Text()
fmt.Println("Xpath语法定位元素文本：", t8)
```
### 网页元素操作
网页元素定位之后，下一步对元素执行操作：

1. Click()：单击操作，适用于所有元素
2. SendKeys(keys string)：写入文本内容，适用于 input 和 textarea
3. Clear()：清空文本内容
4. Submit()：提交表单
5. MoveTo(xOffset, yOffset)：在元素所在位置上的偏移坐标位置
6. FindElement(by, value string)、FindElements(by, value string)：查找网页元素位置
7. TagName()：获取标签名
8. Text()：获取元素的文本内容
9. IsSelected()：判断元素是否被选中，适用于 checkbox 和 radio
10. IsEnabled()：元素是否可编辑或单击
11. IsDisplayed()：判断元素是否可见
12. GetAttribute(name string)：获取元素的某个属性的值
13. Localtion()：获取元素在网页的坐标位置
14. LocationInView()：将元素显示在网页上并获取元素在网页的坐标位置
15. Size()：获取 css 的 width 和 height
16. CssProperty(name string)：获取元素 css 的属性值
17. Screenshot(scroll bool)：截图
```go
// 访问网址
wd.Get("https://www.baidu.com/s?wd=go")
time.Sleep(3 * time.Second)

// 通过 class 属性定位元素
var el1, _ = wd.FindElement(selenium.ByClassName, "s_ipt")

el1.Clear()
el1.SendKeys("Golang")
time.Sleep(3 * time.Second)

var el2, _ = wd.FindElement(selenium.ByID, "su")
// 鼠标移动到元素
el2.MoveTo(0, 0)
time.Sleep(3 * time.Second)
// 单击元素
el2.Click()
// 单击网页元素，Submit()用于表单按钮的单击
el2.Submit()
var tagName, _ = el2.TagName()
fmt.Println("获取网页元素的HTML标签：", tagName)
// 判断元素是否被选中
var r, _ = el2.IsSelected()
fmt.Println("判断元素是否被选中：", r)
// 判断网页元素是否可编辑或可点击
var r1, _ = el2.IsEnabled()
fmt.Println("判断网页元素是否可编辑或可点击：", r1)
var r2, _ = el2.IsDisplayed()
fmt.Println("判断网页元素是否可见：", r2)
// 获取元素的 class 值
var classVal, _ = el2.GetAttribute("class")
fmt.Println("获取元素的 class 值：", classVal)
time.Sleep(3 * time.Second)
// 获取元素的坐标
var location, _ = el2.Location()
fmt.Println("获取元素的坐标：", location)
// 显示并获取元素的坐标
var showLocation, _ = el2.LocationInView()
fmt.Println("显示并获取元素的坐标：", showLocation)
// 获取网页元素的大小
var size, _ = el2.Size()
fmt.Println("获取网页元素的大小：", size)
// 获取 css 样式
var font, _ = el2.CSSProperty("font-size")
fmt.Println("获取 css 样式：", font)
var b, _ = el2.Screenshot(true)
// 保存图片
os.WriteFile("screenshot.jpg", b, 0755)
```
### 浏览器的常用操作
网页自动化测试开发除了定位元素和操作元素之外，还定义了一些常用浏览器操作，比如获取当前网址、网页截图、获取网页标题、刷新网页、执行JS脚本代码等。
```go
// 访问网址
wd.Get("https://www.baidu.com/s?wd=go")
time.Sleep(3 * time.Second)

// 获取当前网址
var url, _ = wd.CurrentURL()
fmt.Println("当前地址：", url)
// 网页截图
var b, _ = wd.Screenshot()
_ = os.WriteFile("screenshot.jpg", b, 0666)
// 获取网页标题
var title, _ = wd.Title()
fmt.Println("当前标题：", title)
// 刷新网页
wd.Refresh()
// 执行 js 脚本实现网页元素操作
var e, _ = wd.FindElement(selenium.ByID, "kw")
wd.ExecuteScript("arguments[0].click()", []any{e})
time.Sleep(3 * time.Second)
```
### 网页加载等待
由于网络延时问题，当使用浏览器访问网站的时候，浏览器都会等待网页加载，只有网页加载完成后，我们才能执行网页操作。
在自动化测试开发中，如果网页加载速度比不上程序执行速度，程序就会因无法定位或操作网页元素而出现异常，为了解决网络延时和程序执行的同步问题，Selenium提供了网页加载等待功能,它是在某个时间范围内等待网页加载，如果超出这个时间范围或网页加载完成,程序终止等待,继续往下执行。
tebeka/selenium定义接口方法WaitWithTimeoutAndInterval()、WaitWithTimeoutO和Wai()实现网页加载等待。一般情况下，只要掌握WaitWithTimeout()和Wait()的使用即可，因为它们都是在WaitWithTimeoutAndinterval()的基础上进行封装的。
```go
// 等待网页元素加载
wd.WaitWithTimeout(func(wd selenium.WebDriver) (bool, error) {
  _, err := wd.FindElement(selenium.ByID, "su")
  if err == nil {
    return true, nil
  } else {
    return false, nil
  }
}, 60*time.Second)

// 等待网页元素加载，默认一分钟
wd.Wait(func(wd selenium.WebDriver) (bool, error) {
  _, err := wd.FindElement(selenium.ByID, "su")
  if err == nil {
    return true, nil
  } else {
    return false, nil
  }
})
```
### iframe与标签页切换
```go
// 切换 iframe 标签
wd.SwitchFrame("useditor_0")
// 定位 iframe 标签中的元素
wd.FindElement(selenium.ByID, "p")
// 切换为主页面
wd.SwitchFrame(nil)

// 切换浏览器标签页
var tabs, _ = wd.WindowHandles()
fmt.Println("所有标签页：", tabs)
// 获取浏览器当前正在显示的标签页
var currentTab, _ = wd.CurrentWindowHandle()
fmt.Println("当前标签页：", currentTab)
// 切换标签页
wd.SwitchWindow(tabs[len(tabs)-1])
```
### Cookie 读写
Selenium使用Cookie主要解决网站多次登录问题，但第一次操控网站仍需通过登录网站获取Cookie，在用户登录状态下获取和保存Cookie，只要在Cookie有效期内多次访问网站即可实现免登录。
目前很多网站为了保护用户数据安全，防止网络爬虫爬取网站数据，在用户登录时会设置验证码、短信验证等反爬功能。如果使用Selenium爬取网站数据，用户登录的验证码功能则成为自动化开发的难点之一。
```go
// 访问网址
wd.Get("https://www.baidu.com/s?wd=go")
time.Sleep(3 * time.Second)

// 获取所有 cookies
var cookies, _ = wd.GetCookies()
for _, v := range cookies {
  fmt.Println("Cookie 信息：", v)
}

// 将 cookie 写入json文件
var fs, _ = os.OpenFile("cookies.json", os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0755)
var encoder = json.NewEncoder(fs)
encoder.Encode(cookies)
fs.Close()

// 删除所有 cookies
wd.DeleteAllCookies()

// 添加 cookies 信息
var f1, _ = os.OpenFile("cookies.json", os.O_RDWR|os.O_CREATE, 0755)
var myCookies []selenium.Cookie
var data = json.NewDecoder(f1)
data.Decode(&myCookies)
f1.Close()

// 写入 json 的 cookies 信息
for _, v := range myCookies {
  wd.AddCookie(&v)
}

var cookies1, _ = wd.GetCookies()
fmt.Println("new Cookie 信息!!!!!!!!!!!!")
for _, v := range cookies1 {
  fmt.Println("Cookie 信息：", v)
}
```
## 爬虫
### HTTP与HTTPS
Http 是一个客户端和服务端请求和应答的标准。客户端是终端用户，服务端是网站。通过 web 浏览器、网络爬虫或者其他工具，客户端发起一个到服务器指定端口的 http 请求，这个客户端叫用户代理（User Agent）。响应的服务器上存储着资源，比如 html 和图片，这个服务器是源服务器（Origin Server），在用户代理和服务器中间可能存在多个中间层，比如代理、网关或者隧道。
通常，由HTTP客户端发起一个请求，建立一个到服务器指定端口（默认是80端口）的TCP连接，HTTP服务器则在那个端口监听客户端发送过来的请求，一旦收到请求，服务器（向客户端)发回一个状态行（比如“HTTP/1.1200OK"）和（响应的)消息，消息的消息体可能是请求的文件、错误消息或者其他一些信息。
HTTP传输的数据都是未加密的，也就是明文的数据，因此使用HTTP传输隐私信息非常不安全。为了保证这些隐私数据能加密传输，于是网景公司设计了SSL(Secure Sockets Layer）协议用于对HTTP传输的数据进行加密，从而诞生了HTTPS。
HTTPS `(Hyper Text Transfer Protocol over Secure Socket Layer，可以理解为HTTP+SSL/TLS)`在传输数据之前需要客户端（浏览器）与服务端（网站）之间进行一次握手，在握手过程中将确立双方加密传输数据的密码信息。
HTTP与HTTPS的区别：
![image.png](https://cdn.nlark.com/yuque/0/2024/png/21870146/1712467167652-ba89d23f-6918-4351-960c-109a721a5f77.png#averageHue=%239f9c95&clientId=u172918ca-6ab8-4&from=paste&height=373&id=u48bdeff1&originHeight=746&originWidth=2267&originalType=binary&ratio=2&rotation=0&showTitle=false&size=1428205&status=done&style=none&taskId=ufdac60ca-494b-4756-84e1-d64e6fee96b&title=&width=1133.5)
HTTPS的SSL中使用了非对称加密、对称加密以及HASH算法。握手过程描述如下：

1. 浏览器将自己支持的一套加密规则发送给网站
2. 网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含网站地址、加密公钥以及证书的颁发机构等信息。
3. 获得网站证书之后，浏览器做以下工作：
   1. 验证证书的合法性（颁发证书的机构是否合法、整数中包含的网站地址是否与正在访问的地址一致等），如果证书受到信任，浏览器就会显示一个小锁头，否则就会给出证书不受信任的提示
   2. 如果证书受信任或者用户接受了不受信任的证书，浏览器就会生成一串随机数的密码，并用证书中提供的公钥加密
   3. 使用约定好的HASH计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有消息发送给网站
4. 网站接受浏览器发来的数据之后要进行以下操作：
   1. 使用自己的私钥将信息解密并取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致
   2. 使用密码加密一段握手消息，发送给浏览器
5. 如果浏览器解密并计算握手消息的HASH与服务端发来的HASH一致，此时握手过程结束，之后所有的通信数据将使用之前浏览器生成的随机密码，并利用对称加密算法进行解密。

浏览器与网站互相发送加密的握手消息并验证，目的是保证双方都获得一致的密码，并且可以正常地加密、解密数据，为真正数据的传输做一次测试。HTTPS一般使用的加密与HAS旺算法如下:
1）非对称加密算法：RSA、DSA/DSS。
2）对称加密算法：AES、RC4、3DES。
3）HASH算法：MD5、SHA1、SHA256。
其中，非对称加密算法用于在握手过程中加密生成的密码，对称加密算法用于对真正传输的数据进行加密，而HASH算法用于验证数据的完整性。由于浏览器生成的密码是整个数据加密的关键，因此在传输的时候使用非对称加密算法对其加密。非对称加密算法会生成公钥和私钥，公钥只能用于加密数据，可以随意传输，而网站的私钥用于对数据进行解密，所以网站都会非常小心地保管自己的私钥，防止泄漏。
SSL握手过程中有任何错误都会使加密连接断开，从而阻止隐私信息的传输，正是由于HTTPS非常安全，攻击者无法从中找到下手的地方，因此更多地采用假证书的手法来欺骗客户端，从而获取明文的信息。
### 请求头
检测请求头是常见的反爬虫策略，因为服务器会对请求头做一次检测来判断这次请求是人为的还是非人为的。为了形成一个良好的代码编写规范，无论网站是否设置Headers反爬虫机制，最好每次发送请求都添加请求头。
请求头的参数如下：

1. Accept:text/html、image/*（浏览器可以接收的文件类型）
2. Accept-Charset：ISO-8859-1（浏览器可以接收的编码类型)
3. Accept-Encoding：gzip,compress（浏览器可以接收的压缩编码类型）
4. Accept-Language:en-us,zh-cn（浏览器可以接收的语言和国家类型）
5. Host：请求的主机地址和端口。
6. If-Modified-Since: Tue, 11 Jul 200018：23:51 GMT（某个页面的缓存时间）
7. Referer：请求来自于哪个页面的URL。
8. User-Agent：代表浏览器相关版本信息。
9. Cookie：浏览器暂存服务器发送的信息。
10. Connection: close(1.0)/Keep-Alive(1.1)（HTTP请求版本的特点）
11. Date: Tue, 11 Jul 2000 18:23:51 GMT（请求网站的时间)

一个标准的请求基本上都带有以上属性。在网络爬虫中，请求头一定要有User-Agent，其他的属性可以根据实际需求添加，因为反爬虫通常检测请求头的Referer和User-Agent，而Cookie不能添加到请求头。除此之外，还有一些比较特殊的请求头信息，如Upgrade-Insecure-Requests（告诉服务器，浏览器可以处理HTTPS）、X-Requested-With（判断是否为Ajax请求）等。
### 使用 net/http 发送请求
爬虫开发的第一步是向目标数据所在的URL发送HTTP请求，发送HTTP请求必须按照请求信息设置相应数据，如设置请求头和请求参数，只要请求头或请求参数缺少某个必要数据，该请求都会视为无效。
除了设置请求头和请求参数之外，还可以为HTTP请求设置Cookie和代理IP，Cookie代表用户信息，某些HTTP请求必须在用户登录状态下才能访问；代理IP是使用不同的IP访问网站，如果使用同一个IP不断向网站发送请求，网站有可能视为爬虫，从而引发反爬虫机制。
```go
var urls = "https://search.51job.com/list/030200,000000,0000,00,9,99,golang,2,1.html"

var req, _ = http.NewRequest("GET", urls, nil)
// POS请求将请求参数以k1=v1&k2=v2的形式表示
// 再由strings.NewReader()转换格式
// b := strings.NewReader("name=cjb")
// req,_ := http.NewRequest("POST",urls, b)
// 为请求对象NewRequest设置请求头
req.Header.Add("Content-Type", "application/x-www-form-urlencod")
req.Header.Add("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Winx64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/94.0.4606.81 Safari/537.36")
// 设置Cookie
// Cookie以结构体Cookie形式表示
cookie := http.Cookie{Name: "clientcookieid", Value: "121"}
req.AddCookie(&cookie)
// 设置代理IP，代理IP必须以匿名函数表示
// 因为Transport的参数Proxy以国名函数定义
// proxy := func(_ *http.Request) (*url.URL, error) {
// 	return url.Parse("http://xxx.XXX.XXX.XXX:xxxx")
// }
// transport := &http.Transport{Proxy: proxy}
// 在Client中设置参数Transport即可实现代理IP
// client := &http.Client{Transport: transport}
client := &http.Client{}
// 发送HTTP请求
resp, _ := client.Do(req)
// 获取网站响应内容
body, _ := io.ReadAll(resp.Body)
// 将响应内容转换为字符串格式输出
fmt.Printf("获取网站响应内容：%v\n", string(body))
```
### 转码与HTML解析
我们通过HTTP请求获取网站响应内容，下一步对响应内容进行清洗处理。响应内容主要为JSON和HTML格式。
Go语言解析HTML需要由第三方包实现，本书推荐使用第三方包goquery，它是Go语言目前较为流行的HTML解析包之一。
```bash
go get github.com/PuerkitoBio/goquery
go get github.com/axgle/mahonia
```
```go
var urls = "https://search.51job.com/list/030200,000000,0000,00,9,99,golang,2,1.html"

var req, _ = http.NewRequest("GET", urls, nil)
// POS请求将请求参数以k1=v1&k2=v2的形式表示
// 再由strings.NewReader()转换格式
// b := strings.NewReader("name=cjb")
// req,_ := http.NewRequest("POST",urls, b)
// 为请求对象NewRequest设置请求头
req.Header.Add("Content-Type", "application/x-www-form-urlencod")
req.Header.Add("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Winx64) AppleWebKit/537.36 (KHTML, like Gecko)Chrome/94.0.4606.81 Safari/537.36")
// 设置Cookie
// Cookie以结构体Cookie形式表示
cookie := http.Cookie{Name: "clientcookieid", Value: "121"}
req.AddCookie(&cookie)
// 设置代理IP,代理IP必须以匿名函数表示
// 因为Transport的参数Proxy以国名函数定义
// proxy := func(_ *http.Request) (*url.URL, error) {
// 	return url.Parse("http://xxx.XXX.XXX.XXX:xxxx")
// }
// transport := &http.Transport{Proxy: proxy}
// 在Client中设置参数Transport即可实现代理IP
// client := &http.Client{Transport: transport}
client := &http.Client{}
// 发送HTTP请求
resp, _ := client.Do(req)
// 获取网站响应内容
body, _ := io.ReadAll(resp.Body)

// 网页响应内容转码
result := convertToString(string(body), "gbk", "utf-8")

fmt.Println(result)
// 使用第三方包goguery读取HTML代码,读取方式有多种
// NewDocumentFromReader：读取字符串的HTML代码
// NewDocumentFromResponse：读取HTML对象,即net/http的resp.Body
// NewDocument：从网址中直接读取HTML代码
dom, _ := goquery.NewDocumentFromReader(strings.NewReader(result))
// Find()是查找HTML里面所有复合要求的标签
// 如果查找Class="ht”的标签,则使用Find(".ht")
// 如果查找id="ht"的标签,则使用Find("#ht")
// 多个标签使用同一个Class,如div和p标签使用Class="ht"
// 若只需div标签,则使用Find("div[class=ht]")
dom.Find(".j_recommend .title").Each(func(i int, selection *goquery.Selection) {
  v := strings.TrimSpace(selection.Text())
  fmt.Printf("内容：%v\n", v)
})

// 通过多层HTML标签查找,只需在Find里面设置多层标签的Class属性即可
// 首先查找Class="rlk"的标签
// 然后在Class="rlk"的标签中查找a标签
// 因此查找方式为Find(".rlk a"）,每个标签之间使用空格隔开
dom.Find(".rlk a").Each(func(i int, selection *goquery.Selection) {
  // 获取数据
  v1 := strings.TrimSpace(selection.Text())
  fmt.Printf("当前数据：%v\n", v1)
  // 获取数据所在的HTML代码
  v2, _ := selection.Html()
  fmt.Printf("获取数据所在HTML代码：%v\n", v2)
  // 使用Attr获取标签的href属性
  v3, _ := selection.Attr("href")
  fmt.Printf("Attr()获取标签的href属性：%v\n", v3)
  // 使用AttrOr获取标签的href属性
  y4 := selection.AttrOr("href", "")
  fmt.Printf("AttrOr()获取标签的href属性：%v\n", y4)
})
```
